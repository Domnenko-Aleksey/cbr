{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text natasha"
      ],
      "metadata": {
        "id": "sQvXlRVJv8uP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e97b46a-2779-4fd9-ee9a-c345c618ac09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (0.14.0)\n",
            "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (2.13.0)\n",
            "Collecting pymorphy2 (from natasha)\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from navec>=0.9.0->natasha) (1.23.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow_text) (0.33.0)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2->natasha)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2->natasha)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2->natasha)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow_text) (0.41.2)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow_text) (3.2.2)\n",
            "Building wheels for collected packages: docopt, intervaltree\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=a1f399653f3f9fb057cc6403b75d3ab8c267c359b778d539d80a0060f69d3d0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26094 sha256=f2d803f0d051479b851dd9dffa149fdabae2c5308bf1555fc29babdcae0c9d10\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built docopt intervaltree\n",
            "Installing collected packages: razdel, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.6.0 yargy-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup  # Для удаления HTML тегов\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.saving import load_model\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "    DatesExtractor,\n",
        "    MoneyExtractor,\n",
        "    AddrExtractor,\n",
        "    Doc\n",
        ")\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "import joblib"
      ],
      "metadata": {
        "id": "VpTyGhmgwHFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset_import"
      ],
      "metadata": {
        "id": "inlmNcgPbRBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# вход на файл\n",
        "drive.mount(\"/content/drive\")\n",
        "DIR = '/content/drive/My Drive/Colab Notebooks/'\n",
        "df = pd.read_excel(DIR + \"CRA_train_1200.xlsx\")[:10]"
      ],
      "metadata": {
        "id": "cpc0BVb0wbW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe74273-156e-47d7-bb9b-c1a1087dc8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models loading\n",
        "<b style=\"color:red\"> warning! it cat take a lot of time! </b>"
      ],
      "metadata": {
        "id": "Aey4EJXObQlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cat = load_model(DIR + 'model_cat_2.keras', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "model_rat = load_model(DIR + 'model_rat_2.keras', custom_objects={'KerasLayer': hub.KerasLayer})"
      ],
      "metadata": {
        "id": "LPzopALFUrpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le_cat = joblib.load(DIR + 'label_encoder_7.joblib')\n",
        "le_rat = joblib.load(DIR + 'label_encoder_17.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oLiLWJlUtBG",
        "outputId": "93b3135b-6bdb-4e64-9caf-850c539c8bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inicialize class"
      ],
      "metadata": {
        "id": "gOAOBeuNb7Pl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-9d3N79vuAA"
      },
      "outputs": [],
      "source": [
        "class Nlp:\n",
        "    def __init__(self):\n",
        "        # Natasha\n",
        "        self.segmenter = Segmenter()\n",
        "        self.morph_vocab = MorphVocab()\n",
        "        self.emb = NewsEmbedding()\n",
        "        self.morph_tagger = NewsMorphTagger(self.emb)\n",
        "        self.syntax_parser = NewsSyntaxParser(self.emb)\n",
        "        self.ner_tagger = NewsNERTagger(self.emb)\n",
        "        self.names_extractor = NamesExtractor(self.morph_vocab)\n",
        "        self.dates_extractor = DatesExtractor(self.morph_vocab)\n",
        "        self.money_extractor = MoneyExtractor(self.morph_vocab)\n",
        "        self.addr_extractor = AddrExtractor(self.morph_vocab)\n",
        "\n",
        "\n",
        "    # Очищаем текст регулярными выражениями\n",
        "    def clear_text(self, text):\n",
        "        soup = BeautifulSoup(text)\n",
        "        text = soup.get_text()\n",
        "        text = re.sub(r'(http\\S+)|(www\\S+)|([\\w\\d]+www\\S+)|([\\w\\d]+http\\S+)', '', text)\n",
        "        text = re.sub(r'[\\n\\t]', ' ', text).strip()  # Перенос, табуляция\n",
        "        text = re.sub(r'[^\\w\\d\\s\\.\\,]', ' ', text)  # Только слова, цифры, пробелы, точки и запятые\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Удаляем двойные пробелы\n",
        "        return text\n",
        "\n",
        "\n",
        "    # Извлекаем фичи\n",
        "    def get_features(self, df):\n",
        "        start_time = time.time()\n",
        "        features_list = []\n",
        "        for i in range(df.shape[0]):\n",
        "            ner_list = self.get_ner_features(df.iloc[i])\n",
        "            words = df.iloc[i].split()\n",
        "            count = len(words)  # Количество слов в строке\n",
        "            if count > 0:\n",
        "                average = sum(len(word) for word in words) / count\n",
        "                uniq = round(100*len(set(words))/count)  # % уникальных слов в строке\n",
        "                features_list.append(ner_list + [count, average, uniq])\n",
        "            if (i+1) % 10 == 0:\n",
        "                delta_time = round(time.time() - start_time)\n",
        "                print(f'Обработано {i+1} из {df.shape[0]}, {delta_time}c')\n",
        "        return np.array(features_list)\n",
        "\n",
        "\n",
        "    # В категорийные признаки\n",
        "    def to_categorical(self, df, labels):\n",
        "        le = LabelEncoder()\n",
        "        le.fit(labels)\n",
        "        label = le.transform(df)\n",
        "        return to_categorical(label, num_classes=len(labels), dtype='int')\n",
        "\n",
        "\n",
        "    # Возвращает список с количеством найденных именованных сущностей [names, dates, LOC, MONEY]\n",
        "    def get_ner_features(self, text):\n",
        "        names = len(list(nlp.names_extractor(text)))\n",
        "        money = len(list(nlp.money_extractor(text)))\n",
        "        addr = len(list(nlp.addr_extractor(text)))\n",
        "        return [names, money, addr]\n",
        "\n",
        "\n",
        "    # Извлекает именнованные сущности - имена, названия\n",
        "    def names_extractor(self, text):\n",
        "        return self.names_extractor(text)\n",
        "\n",
        "\n",
        "    # Извлекает именнованные сущности - даты\n",
        "    def dates_extractor(self, text):\n",
        "        return self.dates_extractor(text)\n",
        "\n",
        "\n",
        "    # Извлекает именнованные сущности - деньги\n",
        "    def money_extractor(self, text):\n",
        "        return self.money_extractor(text)\n",
        "\n",
        "\n",
        "    # Извлекает именнованные сущности - локацию, адреса\n",
        "    def addr_extractor(self, text):\n",
        "        return self.addr_extractor(text)\n",
        "\n",
        "    def prediction_pipeline(self, df):\n",
        "      copied_df = df[\"pr_txt\"].copy()\n",
        "      cleared_text_list = list(copied_df.map(self.clear_text))\n",
        "      df_text = pd.DataFrame(cleared_text_list, columns=['text'])\n",
        "      x_features = self.get_features(df_text['text'])\n",
        "      df_features = pd.DataFrame(x_features, columns=['name', 'date', 'location', 'count', 'average', 'uniq'])\n",
        "      pred_cat_v = model_cat.predict([df_text, df_features])\n",
        "      pred_rat_v = model_rat.predict([df_text, df_features])\n",
        "      pred_cat_am = np.argmax(pred_cat_v, axis=1)\n",
        "      pred_rat_am = np.argmax(pred_rat_v, axis=1)\n",
        "      pred_cat_y = le_cat.inverse_transform(pred_cat_am)\n",
        "      pred_rat_y = le_rat.inverse_transform(pred_rat_am)\n",
        "      answer = pd.DataFrame({'Категория': pred_cat_y, 'Уровень рейтинга': pred_rat_y})\n",
        "      out = pd.concat([copied_df, answer], axis=1)\n",
        "      out.to_excel('answer.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = Nlp()"
      ],
      "metadata": {
        "id": "1Y6IwTptwTBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.prediction_pipeline(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXcR6X0PaIS2",
        "outputId": "e35bfca1-57bd-4956-a310-a60f729a8135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обработано 10 из 10, 81c\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "0    Повышение кредитного рейтинга  Акционерного об...\n",
            "1    «Эксперт РА» подтвердил кредитный рейтинг комп...\n",
            "2    НКР повысило кредитный рейтинг ООО \"ОТЭКО-Порт...\n",
            "3    «Эксперт РА» присвоил кредитный рейтинг ПАО «Ф...\n",
            "4    29 марта 2023 г. Ведущий рейтинговый аналитик ...\n",
            "5    Кредитный рейтинг  ПАО «ФосАгро» (далее — Комп...\n",
            "6    «Эксперт РА» повысил кредитный рейтинг ОАО «МР...\n",
            "7    «Эксперт РА» понизил кредитный рейтинг ПАО «М....\n",
            "8    «Эксперт РА» повысил кредитный рейтинг компани...\n",
            "9    Кредитный рейтинг  ООО «МВМ»  (далее — Компани...\n",
            "Name: pr_txt, dtype: object\n",
            "  Категория Уровень рейтинга\n",
            "0         A                A\n",
            "1        BB               BB\n",
            "2         A                A\n",
            "3       AAA              AAA\n",
            "4       BBB              BBB\n",
            "5       AAA              AAA\n",
            "6        AA              AA+\n",
            "7         A                A\n",
            "8        BB              BB+\n",
            "9         A                A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXij2sKDtbGA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}